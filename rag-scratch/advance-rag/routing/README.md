# Routing RAG

This directory contains implementations and examples of Routing RAG techniques.

## Contents

- `routing-rag.ipynb`: Jupyter notebook demonstrating implementation of routing strategies for RAG systems
- `llm-generated-collections.png`: Visualization of collections generated by LLM
- `routing-flow.png`: Diagram showing the RAG flow with routing
- `logical-router.png`: Diagram showing the logical routing flow

## Overview

Routing RAG is an advanced technique for Retrieval Augmented Generation that intelligently routes queries to appropriate knowledge sources or retrieval strategies based on the query type. This approach improves response quality by ensuring the most relevant information is retrieved for each query.

## What is Routing RAG?

Routing RAG is focused on directing queries to the most appropriate knowledge collections or retrieval methods. The implementation in this repository demonstrates:

1. **Data Ingestion**: The notebook shows how to scrape content from a website (chaidocs.vercel.app) to create a test dataset with different sections
2. **Collection Creation**: Content is organized into collections based on section titles (e.g., chai-aur-html, chai-aur-git, chai-aur-c++, etc.)
3. **Dynamic Section Extraction**: The notebook programmatically extracts section titles to create separate collections
4. **Query Routing**: Implements logic to route user queries to the most relevant collection based on query content

## Implementation Details

The routing-rag.ipynb file demonstrates:

- Web scraping with BeautifulSoup and langchain_community document loaders
- Processing document metadata to organize content into logical collections
- Using OpenAI embeddings for vector storage
- Implementing a routing mechanism to direct queries to the appropriate collection
- Setting up a vector database with multiple collections

## Detailed Workflow Process

### Data Scraping and Extraction

1. **Web Scraping Setup**: The notebook uses BeautifulSoup and requests to extract links from a base URL (chaidocs.vercel.app)

   ```python
   def extract_links(url, base_domain="chaidocs.vercel.app"):
       response = requests.get(url)
       soup = BeautifulSoup(response.text, "html.parser")
       links = []
       for a_tag in soup.find_all('a', href=True):
           href = a_tag['href']
           full_url = urljoin(url, href)
           if base_domain in full_url:
               links.append(full_url)
       return links
   ```

2. **Content Loading**: Each extracted link is loaded using WebBaseLoader from langchain_community

   ```python
   all_documents = []
   for link in all_links:
       try:
           loader = WebBaseLoader(link)
           documents = loader.load()
           all_documents.extend(documents)
           print(f"Loaded: {link}")
       except Exception as e:
           print(f"Error loading {link}: {e}")
   ```

3. **Section Identification**: The notebook analyzes document metadata to dynamically identify document sections

   ```python
   section_titles = []
   for index in range(len(all_documents)):
       title = all_documents[index].metadata.get('source').split('/')[-3]

       if("chai-aur") in title:
           section_titles.append(title)
   ```

### Vector Store Creation

1. **Document Splitting**: Documents are split into chunks for more precise retrieval

   ```python
   text_splitter = RecursiveCharacterTextSplitter(
       chunk_size=1000,
       chunk_overlap=200
   )
   ```

2. **Creating Collections**: Documents are organized into collections based on their section titles

   ```python
   collections = {}
   for title in set(section_titles):
       collection_docs = []
       for doc in all_documents:
           if title in doc.metadata.get('source'):
               collection_docs.append(doc)

       # Split documents for this collection
       split_docs = text_splitter.split_documents(collection_docs)
       collections[title] = split_docs
   ```

3. **Embedding and Vector Store**: Each collection is stored in a separate vector store using OpenAI embeddings

   ```python
   embeddings = OpenAIEmbeddings()
   vector_stores = {}

   for collection_name, docs in collections.items():
       vector_stores[collection_name] = QdrantVectorStore.from_documents(
           docs,
           embeddings,
           collection_name=collection_name
       )
   ```

### LLM-Based Collection Routing

1. **LLM for Collection Selection**: The notebook uses an LLM to determine which collection is most relevant for a query

   ```python
   def get_collection_for_query(query):
       collection_names = list(collections.keys())
       prompt = f"""Given a user query and a list of knowledge base collections, determine which collection would be most appropriate to answer the query.

       Collections: {collection_names}
       User Query: {query}

       Name of the most appropriate collection:"""

       client = OpenAI()
       response = client.chat.completions.create(
           model="gpt-3.5-turbo",
           messages=[{"role": "user", "content": prompt}]
       )
       return response.choices[0].message.content.strip()
   ```

2. **Query Execution**: Once the appropriate collection is determined, the query is directed to that specific collection

   ```python
   def route_and_answer_query(query):
       collection_name = get_collection_for_query(query)
       print(f"Routing query to collection: {collection_name}")

       if collection_name in vector_stores:
           retriever = vector_stores[collection_name].as_retriever()
           retrieved_docs = retriever.get_relevant_documents(query)

           # Process retrieved documents and generate answer...
           return answer
       else:
           return "Sorry, I couldn't find an appropriate collection to answer your query."
   ```

## Use Cases

Routing RAG is particularly useful when:

- You have diverse knowledge sources covering different domains
- Queries clearly belong to specific topic areas
- You want to improve retrieval precision by narrowing the search space
- You need to direct queries to specialized retrieval strategies
